\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Eigenvalue Decomposition}{1}{section.1}\protected@file@percent }
\newlabel{fig:eigen1}{{1}{2}{Eigenvalue Decomposition}{section.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Conceptual illustration of eigenvalue decomposition for a square operator \(A\). The transformation \(A\vec  {x} = \lambda \vec  {x}\) identifies special directions \(\vec  {x}_i\) (eigenvectors) that are simply scaled by the corresponding eigenvalues \(\lambda _i\). Diagonalizing \(A\) via its eigenvalues and eigenvectors reveals the natural coordinate system in which the action of the operator is decoupled.}}{2}{figure.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Full Singular Value Decomposition}{2}{section.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Schematic representation of the full singular value decomposition (SVD). Any \(m \times n\) matrix \(A\) can be factored into the product \(A = U S V^{T}\), where \(U\) and \(V\) are orthonormal matrices containing the left and right singular vectors, respectively, and \(S\) contains the singular values. This decomposition expresses the linear transformation encoded in \(A\) as a rotation, scaling, and a second rotation.}}{2}{figure.2}\protected@file@percent }
\newlabel{fig:eigen1p5}{{2}{2}{Schematic representation of the full singular value decomposition (SVD). Any \(m \times n\) matrix \(A\) can be factored into the product \(A = U S V^{T}\), where \(U\) and \(V\) are orthonormal matrices containing the left and right singular vectors, respectively, and \(S\) contains the singular values. This decomposition expresses the linear transformation encoded in \(A\) as a rotation, scaling, and a second rotation}{figure.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}From Full SVD to Thin SVD}{3}{section.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Thin singular value decomposition (SVD) for a data matrix \(A \in \mathbb  {R}^{m \times n}\). Here \(A\) is decomposed as \(A = U_n S_n V^{T}\), where the columns of \(U_n\) (\(u_1, u_2, \dots  , u_n\)) represent orthonormal basis functions in the measurement space, \(S_n\) contains the nonzero singular values, and the rows of \(V^{T}\) (\(v_1, v_2, \dots  , v_n\)) encode the amplitudes or time dependences of each component. This form reflects the dimensionally reduced version of the full SVD.}}{4}{figure.3}\protected@file@percent }
\newlabel{fig:eigen2}{{3}{4}{Thin singular value decomposition (SVD) for a data matrix \(A \in \mathbb {R}^{m \times n}\). Here \(A\) is decomposed as \(A = U_n S_n V^{T}\), where the columns of \(U_n\) (\(u_1, u_2, \dots , u_n\)) represent orthonormal basis functions in the measurement space, \(S_n\) contains the nonzero singular values, and the rows of \(V^{T}\) (\(v_1, v_2, \dots , v_n\)) encode the amplitudes or time dependences of each component. This form reflects the dimensionally reduced version of the full SVD}{figure.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Low-Rank Approximation}{4}{section.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Application of the thin SVD to a two-dimensional stopped-flow spectroscopic dataset \(A_{m \times n}\) (e.g., wavelength vs.\ pump--probe delay). The matrix is decomposed into spectral basis vectors \(u_1, u_2, \dots  \), singular values \(s_i\), and temporal components \(v_i\). The left singular vectors describe orthogonal spectral signatures, while the right singular vectors capture their corresponding time evolutions, providing a compact representation of the dataset.}}{5}{figure.4}\protected@file@percent }
\newlabel{fig:eigen3}{{4}{5}{Application of the thin SVD to a two-dimensional stopped-flow spectroscopic dataset \(A_{m \times n}\) (e.g., wavelength vs.\ pump--probe delay). The matrix is decomposed into spectral basis vectors \(u_1, u_2, \dots \), singular values \(s_i\), and temporal components \(v_i\). The left singular vectors describe orthogonal spectral signatures, while the right singular vectors capture their corresponding time evolutions, providing a compact representation of the dataset}{figure.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}PCA via the Covariance Matrix}{6}{section.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Illustration of principal component analysis (PCA) for a two-dimensional dataset. The centered data points (light blue) exhibit correlated variation. The black arrows represent the first and second principal components, obtained from the singular value decomposition of the data matrix. The first principal component aligns with the direction of maximum variance, while the second spans the orthogonal subspace capturing the remaining variance.}}{6}{figure.5}\protected@file@percent }
\newlabel{fig:pca}{{5}{6}{Illustration of principal component analysis (PCA) for a two-dimensional dataset. The centered data points (light blue) exhibit correlated variation. The black arrows represent the first and second principal components, obtained from the singular value decomposition of the data matrix. The first principal component aligns with the direction of maximum variance, while the second spans the orthogonal subspace capturing the remaining variance}{figure.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Summary}{7}{section.6}\protected@file@percent }
\gdef \@abspage@last{7}
